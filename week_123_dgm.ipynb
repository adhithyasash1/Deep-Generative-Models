{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adhithyasash1/Deep-Generative-Models/blob/main/week_123_dgm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1mXABz7jKo_"
      },
      "source": [
        "# Conditional DC GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dpyk0_qgioXH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hr-bezzViq0A"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "latent_dim = 100             # Size of input noise vector to generator\n",
        "num_classes = 10             # MNIST has 10 digit classes\n",
        "embedding_dim = 50           # Size of label embedding vectors\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "img_size = 28                # MNIST image size\n",
        "channels = 1                 # Grayscale\n",
        "img_shape = (channels, img_size, img_size)\n",
        "\n",
        "# Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(img_size),                # Just in case\n",
        "    transforms.ToTensor(),                      # Converts to [C, H, W]\n",
        "    transforms.Normalize([0.5], [0.5])          # Scale to [-1, 1]\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_Wch2G8Piq6q"
      },
      "outputs": [],
      "source": [
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, embedding_dim)\n",
        "        self.init_size = img_size // 4  # Output size after 2 upscales\n",
        "        self.l1 = nn.Sequential(nn.Linear(latent_dim + embedding_dim, 128 * self.init_size ** 2))\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),  # 7 → 14\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.ReLU(True),\n",
        "            nn.Upsample(scale_factor=2),  # 14 → 28\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        label_input = self.label_emb(labels)\n",
        "        x = torch.cat((noise, label_input), dim=1)\n",
        "        x = self.l1(x)\n",
        "        x = x.view(x.size(0), 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(x)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "U1fcY2KIiuOq"
      },
      "outputs": [],
      "source": [
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, embedding_dim)\n",
        "        self.label_proj = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, img_size * img_size)\n",
        "        )\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(2, 64, 3, 2, 1),  # (2, 28, 28) → (64, 14, 14)\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 3, 2, 1),  # (64, 14, 14) → (128, 7, 7)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 7 * 7, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        label_input = self.label_emb(labels)\n",
        "        label_map = self.label_proj(label_input).view(img.size(0), 1, img_size, img_size)\n",
        "        d_in = torch.cat((img, label_map), dim=1)  # Concatenate channel-wise\n",
        "        validity = self.model(d_in)\n",
        "        return validity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "abZt_ypEiuV9"
      },
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Loss and Optimizers\n",
        "criterion = nn.BCELoss()\n",
        "opt_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "opt_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "\n",
        "# Image saving utility\n",
        "def sample_image(epoch, generator, noise, labels):\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        gen_imgs = generator(noise, labels).detach().cpu()\n",
        "        gen_imgs = gen_imgs * 0.5 + 0.5  # Denormalize to [0,1]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(15, 2))\n",
        "    for i in range(10):\n",
        "        axes[i].imshow(gen_imgs[i][0], cmap=\"gray\")\n",
        "        axes[i].set_title(f\"Digit: {labels[i].item()}\")\n",
        "        axes[i].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    os.makedirs(\"images\", exist_ok=True)\n",
        "    plt.savefig(f\"images/cdcgan_epoch_{epoch}.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# Fixed noise and labels for consistent evaluation\n",
        "fixed_noise = torch.randn(10, latent_dim).to(device)\n",
        "fixed_labels = torch.arange(0, 10).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvW9nQfyi3PX",
        "outputId": "cd2bc71a-4ad1-42fe-f560-391dd49f7654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/10] D loss: 0.5599 | G loss: 0.9241\n",
            "[Epoch 2/10] D loss: 0.4916 | G loss: 1.0864\n",
            "[Epoch 3/10] D loss: 0.5625 | G loss: 1.3506\n",
            "[Epoch 4/10] D loss: 0.3190 | G loss: 2.2019\n",
            "[Epoch 5/10] D loss: 0.3693 | G loss: 0.9147\n",
            "[Epoch 6/10] D loss: 0.3898 | G loss: 1.3725\n",
            "[Epoch 7/10] D loss: 0.3671 | G loss: 1.2043\n",
            "[Epoch 8/10] D loss: 0.6103 | G loss: 0.6295\n",
            "[Epoch 9/10] D loss: 0.3689 | G loss: 1.6296\n",
            "[Epoch 10/10] D loss: 0.6969 | G loss: 0.7799\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for i, (imgs, labels) in enumerate(train_loader):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        batch_size = imgs.size(0)\n",
        "\n",
        "        real = torch.ones(batch_size, 1).to(device)\n",
        "        fake = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # Train Generator\n",
        "        opt_G.zero_grad()\n",
        "        z = torch.randn(batch_size, latent_dim).to(device)\n",
        "        gen_labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
        "        gen_imgs = generator(z, gen_labels)\n",
        "        validity = discriminator(gen_imgs, gen_labels)\n",
        "        g_loss = criterion(validity, real)\n",
        "        g_loss.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "        # Train Discriminator\n",
        "        opt_D.zero_grad()\n",
        "        real_loss = criterion(discriminator(imgs, labels), real)\n",
        "        fake_loss = criterion(discriminator(gen_imgs.detach(), gen_labels), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "    print(f\"[Epoch {epoch}/{epochs}] D loss: {d_loss.item():.4f} | G loss: {g_loss.item():.4f}\")\n",
        "    sample_image(epoch, generator, fixed_noise, fixed_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DCGAN with Recent Advances"
      ],
      "metadata": {
        "id": "_uzGmq_SukPc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "V2eYZIusi3Rs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KxDjJxnLi3UF"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "num_classes = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fj-9QyjZiXcV"
      },
      "outputs": [],
      "source": [
        "class ConditionalBatchNorm(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(num_features, affine=False)\n",
        "        self.embed = nn.Embedding(num_classes, num_features * 2)\n",
        "        self.embed.weight.data[:, :num_features].fill_(1.0)  # gamma\n",
        "        self.embed.weight.data[:, num_features:].zero_()     # beta\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = self.bn(x)\n",
        "        gamma, beta = self.embed(y).chunk(2, dim=1)\n",
        "        gamma = gamma.unsqueeze(2).unsqueeze(3)\n",
        "        beta = beta.unsqueeze(2).unsqueeze(3)\n",
        "        return gamma * out + beta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.key   = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.value = nn.Conv2d(in_channels, in_channels, 1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "        query = self.query(x).view(B, -1, H*W).permute(0, 2, 1)\n",
        "        key   = self.key(x).view(B, -1, H*W)\n",
        "        value = self.value(x).view(B, -1, H*W)\n",
        "\n",
        "        attention = torch.bmm(query, key)\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
        "        out = out.view(B, C, H, W)\n",
        "        return self.gamma * out + x"
      ],
      "metadata": {
        "id": "rQFn8MAgyMIx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, img_channels, feature_maps, num_classes):\n",
        "        super().__init__()\n",
        "        self.label_embed = nn.Embedding(num_classes, z_dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, feature_maps*4, 4, 1, 0),  # 1x1 -> 4x4\n",
        "            nn.BatchNorm2d(feature_maps*4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(feature_maps*4, feature_maps*2, 4, 2, 1),  # 4x4 -> 8x8\n",
        "            nn.BatchNorm2d(feature_maps*2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            SelfAttention(feature_maps*2),  # 8x8\n",
        "            nn.ConvTranspose2d(feature_maps*2, feature_maps, 4, 2, 1),  # 8x8 -> 16x16\n",
        "            nn.BatchNorm2d(feature_maps),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(feature_maps, img_channels, 4, 2, 1),  # 16x16 -> 32x32\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "        z = z + self.label_embed(labels)\n",
        "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
        "        return self.net(z)"
      ],
      "metadata": {
        "id": "94t6Guf6yMLD"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_channels, feature_maps, num_classes, embedding_dim=128):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(num_classes, embedding_dim)\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(img_channels, feature_maps, 4, 2, 1),  # 28 → 14\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(feature_maps, feature_maps * 2, 4, 2, 1),  # 14 → 7\n",
        "            nn.LeakyReLU(0.2),\n",
        "            SelfAttention(feature_maps * 2),  # 7x7\n",
        "            nn.Conv2d(feature_maps * 2, feature_maps * 4, 3, 2, 1),  # 7 → 4\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "        # Dynamically compute the output size after convs\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, img_channels, 28, 28)\n",
        "            dummy_output = self.features(dummy_input)\n",
        "            self.flat_dim = dummy_output.view(1, -1).shape[1]\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.real_fake = nn.Linear(self.flat_dim, 1)\n",
        "        self.embedding_proj = nn.Linear(self.flat_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        x = self.features(x)\n",
        "        x = self.flatten(x)\n",
        "        out_rf = self.real_fake(x)\n",
        "\n",
        "        class_embed = self.embed(labels)\n",
        "        proj = torch.sum(self.embedding_proj(x) * class_embed, dim=1, keepdim=True)\n",
        "\n",
        "        return out_rf + proj, self.embedding_proj(x)"
      ],
      "metadata": {
        "id": "hcrAlXxRyMNX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_loss(embeddings, labels, temperature=0.1):\n",
        "    normed = F.normalize(embeddings, dim=1)\n",
        "    sim_matrix = torch.matmul(normed, normed.T)\n",
        "    labels = labels.unsqueeze(1)\n",
        "    mask = labels == labels.T\n",
        "\n",
        "    sim_matrix = sim_matrix / temperature\n",
        "    sim_matrix_exp = torch.exp(sim_matrix)\n",
        "    sim_matrix_exp = sim_matrix_exp * (~torch.eye(len(labels), dtype=bool, device=labels.device))\n",
        "\n",
        "    positives = sim_matrix_exp * mask\n",
        "    negatives = sim_matrix_exp * ~mask\n",
        "\n",
        "    loss = -torch.log(positives.sum(1) / negatives.sum(1) + 1e-8).mean()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "WalhvpHsyMPg"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = Generator(z_dim=100, img_channels=1, feature_maps=64, num_classes=10).to(device)\n",
        "D = Discriminator(img_channels=1, feature_maps=64, num_classes=10).to(device)\n",
        "\n",
        "opt_G = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "opt_D = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "lambda_contrast = 1.0\n",
        "\n",
        "for epoch in range(5):\n",
        "    for imgs, labels in dataloader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        B = imgs.size(0)\n",
        "        z = torch.randn(B, 100).to(device)\n",
        "        fake = G(z, labels)\n",
        "\n",
        "        # Train Discriminator\n",
        "        D_real, emb_real = D(imgs, labels)\n",
        "        D_fake, emb_fake = D(fake.detach(), labels)\n",
        "\n",
        "        real_loss = loss_fn(D_real, torch.ones_like(D_real))\n",
        "        fake_loss = loss_fn(D_fake, torch.zeros_like(D_fake))\n",
        "        contrast_loss = contrastive_loss(emb_real, labels)\n",
        "\n",
        "        d_loss = real_loss + fake_loss + lambda_contrast * contrast_loss\n",
        "        opt_D.zero_grad()\n",
        "        d_loss.backward()\n",
        "        opt_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        D_fake, _ = D(fake, labels)\n",
        "        g_loss = loss_fn(D_fake, torch.ones_like(D_fake))\n",
        "        opt_G.zero_grad()\n",
        "        g_loss.backward()\n",
        "        opt_G.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}: D Loss {d_loss.item():.3f}, G Loss {g_loss.item():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy71UvMVyMR-",
        "outputId": "9ba1c87f-e834-42cb-b1ce-c946cd40afe8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: D Loss -6.041, G Loss 5.036\n",
            "Epoch 1: D Loss -6.246, G Loss 5.856\n",
            "Epoch 2: D Loss -6.466, G Loss 7.188\n",
            "Epoch 3: D Loss -7.053, G Loss 9.577\n",
            "Epoch 4: D Loss -6.596, G Loss 9.563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_generated_images(generator, n_images=16):\n",
        "    z = torch.randn(n_images, 100).to(device)\n",
        "    labels = torch.randint(0, 10, (n_images,)).to(device)\n",
        "    with torch.no_grad():\n",
        "        samples = generator(z, labels).cpu()\n",
        "    grid = make_grid(samples, nrow=4, normalize=True)\n",
        "    plt.imshow(grid.permute(1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wl3VVgrcyZU9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_generated_images(generator, n_images=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "p2G4K5dJyb0m",
        "outputId": "2eefab03-f45f-4d25-edb5-f1ca1def3069"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-q2p1xHcyb3G"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j5uNM5Y4yb5c"
      },
      "execution_count": 41,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q1mXABz7jKo_"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOlQeuni2ZXtdsaDjJTNUw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}